{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNj6S+zGP0Rebe6N4PzGUOI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sayali19-cell/RAG-Evaluation/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from sklearn.metrics import precision_score\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(\"RAG Evaluation\")\n",
        "\n",
        "ground_truth_docs = {\"query1\": \"This is a relevant document.\", \"query2\": \"Another important document.\"}\n",
        "retrieved_docs = {\"query1\": \"This is a relevant document.\", \"query2\": \"An unrelated document.\"}\n",
        "\n",
        "ground_truth_responses = {\"query1\": \"The capital of France is Paris.\", \"query2\": \"Water boils at 100 degrees Celsius.\"}\n",
        "generated_responses = {\"query1\": \"The capital of France is Paris.\", \"query2\": \"Water boils at 150 degrees Celsius.\"}\n",
        "\n",
        "# 1. Retrieval Evaluation (Precision)\n",
        "def evaluate_retrieval(ground_truth_docs, retrieved_docs):\n",
        "    \"\"\"\n",
        "    Evaluates retrieval precision.\n",
        "    \"\"\"\n",
        "    true_positive = sum(1 for q in ground_truth_docs if ground_truth_docs[q] == retrieved_docs[q])\n",
        "    total_retrieved = len(retrieved_docs)\n",
        "    precision = true_positive / total_retrieved if total_retrieved > 0 else 0\n",
        "    logger.info(f\"Retrieval Precision: {precision:.2f}\")\n",
        "    return precision\n",
        "\n",
        "# 2. Generation Evaluation (BLEU Score)\n",
        "def evaluate_generation(ground_truth_responses, generated_responses):\n",
        "    \"\"\"\n",
        "    Evaluates generation quality using BLEU scores.\n",
        "    \"\"\"\n",
        "    bleu_scores = []\n",
        "    for query, true_response in ground_truth_responses.items():\n",
        "        generated = generated_responses.get(query, \"\")\n",
        "        bleu = sentence_bleu([true_response.split()], generated.split())\n",
        "        bleu_scores.append(bleu)\n",
        "        logger.info(f\"Query: {query}, BLEU Score: {bleu:.2f}\")\n",
        "    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
        "    logger.info(f\"Average BLEU Score: {avg_bleu:.2f}\")\n",
        "    return avg_bleu\n",
        "\n",
        "def main():\n",
        "    logger.info(\"Starting RAG Evaluation...\")\n",
        "    retrieval_precision = evaluate_retrieval(ground_truth_docs, retrieved_docs)\n",
        "    generation_bleu = evaluate_generation(ground_truth_responses, generated_responses)\n",
        "\n",
        "    logger.info(\"RAG Evaluation Completed.\")\n",
        "    logger.info(f\"Final Results - Retrieval Precision: {retrieval_precision:.2f}, Generation BLEU Score: {generation_bleu:.2f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNCRxTifWisH",
        "outputId": "0a6542c3-0957-4a4c-c40c-9bf2bb403ffe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/RAG.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbTrb03FYXF7",
        "outputId": "0e546651-9c02-4d69-bf2e-097b6306cdba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:RAG Evaluation:Starting RAG Evaluation...\n",
            "INFO:RAG Evaluation:Retrieval Precision: 0.50\n",
            "INFO:RAG Evaluation:Query: query1, BLEU Score: 1.00\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "INFO:RAG Evaluation:Query: query2, BLEU Score: 0.00\n",
            "INFO:RAG Evaluation:Average BLEU Score: 0.50\n",
            "INFO:RAG Evaluation:RAG Evaluation Completed.\n",
            "INFO:RAG Evaluation:Final Results - Retrieval Precision: 0.50, Generation BLEU Score: 0.50\n"
          ]
        }
      ]
    }
  ]
}